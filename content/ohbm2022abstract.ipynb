{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of confound removal strategies on functional connectivity generated from fMRIprep preprocessed data \n",
    "\n",
    "Authors\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- Existing literature on confounds has compared the denoising strategies extensively\n",
    "- Popularity of minimally preprocessing pipeline:\n",
    "    - Pros: Freedom to experiment with impact of various factors for method development\n",
    "    - Cons: difficult for end users with less method-focused research to do the downstream analysis right\n",
    "- fMRIprep provides minimal preprocessing pipeline with lots of confounds with the minimally processed data\n",
    "- Itâ€™s difficult to navigate the confounds and implement the sensible subset of variables in downstream analysis\n",
    "- We provide solution and benchmark using fMRIPrep outputs on various modern connectomes\n",
    "\n",
    "## Methods\n",
    "\n",
    "- Dataset: pixar movie watching developmental dataset\n",
    "- fMRIprep version\n",
    "- Nilearn code implementation\n",
    "- Atlases used\n",
    "- Metric: \n",
    "    - correlation of connectivity edges with mean FD\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from metrics import qcfc, compute_pairwise_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path of input and output\n",
    "OUTPUT = \"inputs/interim\"\n",
    "INPUT = \"inputs/dataset-ds000288.tar.gz\"\n",
    "CENTROIDS = \"inputs/atlas/schaefer20187networks/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_2mm.Centroid_RAS.csv\"\n",
    "output = Path.cwd().parents[0] / OUTPUT\n",
    "input_connectomes = Path.cwd().parents[0] / INPUT\n",
    "input_centroids = Path.cwd().parents[0] / CENTROIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(input_connectomes, 'r:gz') as tar:\n",
    "    movement = tar.extractfile(\"dataset-ds000288/dataset-ds000288_desc-movement_phenotype.tsv\").read()\n",
    "    movement = pd.read_csv(io.BytesIO(movement),\n",
    "                            sep='\\t', index_col=0, header=0, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(input_connectomes, 'r:gz') as tar:\n",
    "    # find the strategies we need to iterate through.\n",
    "    benchmark_strategies = []\n",
    "    for member in tar.getmembers():\n",
    "        filename = member.name.split('/')[-1]\n",
    "        if \"data.tsv\" in filename:\n",
    "            strategy = filename.split(\"desc-\")[-1].split(\"_data\")[0]\n",
    "            benchmark_strategies.append(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrubbing\n",
      "compcor\n",
      "simple\n",
      "simple+gsr\n",
      "raw\n",
      "aroma+gsr\n",
      "aroma\n",
      "scrubbing+gsr\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open(input_connectomes, 'r:gz') as tar:\n",
    "    # find the strategies we need to iterate through.\n",
    "    benchmark_strategies = []\n",
    "    for member in tar.getmembers():\n",
    "        filename = member.name.split('/')[-1]\n",
    "        if \"data.tsv\" in filename:\n",
    "            strategy = filename.split(\"desc-\")[-1].split(\"_data\")[0]\n",
    "            benchmark_strategies.append(strategy)\n",
    "\n",
    "    for strategy_name in benchmark_strategies:\n",
    "        print(strategy_name)\n",
    "        connectome = tar.extractfile(f\"dataset-ds000288/atlas-schaefer7networks/dataset-ds000288_atlas-schaefer7networks_nroi-400_desc-{strategy_name}_data.tsv\").read()\n",
    "        dataset_connectomes = pd.read_csv(io.BytesIO(connectome), sep='\\t', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15998/1084376572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectivityMeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnectivities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_connectomes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "cm = ConnectivityMeasure().fit()\n",
    "cm.inverse_transform(connectivities=dataset_connectomes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.95035476e-01,  4.50003888e-01,  3.55539269e-01, ...,\n",
       "         4.04366245e-01,  1.97208727e-01,  3.51785879e-01],\n",
       "       [ 5.51107507e-01,  5.24035590e-01,  4.04788253e-01, ...,\n",
       "         3.88681385e-01,  2.03812205e-01,  3.52432545e-01],\n",
       "       [ 5.34794228e-01,  2.66094344e-01,  2.74350480e-01, ...,\n",
       "         5.73319661e-01,  2.16616869e-01,  5.83886877e-01],\n",
       "       ...,\n",
       "       [            nan,             nan,             nan, ...,\n",
       "                    nan,             nan,             nan],\n",
       "       [-9.29242750e-17,  5.33294170e-01, -9.36010482e-17, ...,\n",
       "         3.64463243e-17,  1.04829695e-32, -9.57597658e-33],\n",
       "       [ 4.28087791e-01,  6.10174002e-01,  3.28547191e-01, ...,\n",
       "         4.68958214e-01,  3.72808304e-01,  4.53677173e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_connectomes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
